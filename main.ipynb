{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EjFbNNl1n3E"
      },
      "source": [
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You\n",
        "# may want to inspect the file for more details.\n",
        "\n",
        "# ! pip install procgen\n",
        "# ! wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
        "\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "\n",
        "\n",
        "# Hyperparameters. These values should be a good starting point. You can modify them later once you have a working\n",
        "# implementation.\n",
        "\n",
        "# Hyperparameters\n",
        "total_steps = 8e6\n",
        "num_envs = 32\n",
        "num_levels = 10\n",
        "num_steps = 256\n",
        "num_epochs = 3\n",
        "batch_size = 512\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_xWVpxq1rNe"
      },
      "source": [
        "# Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder\n",
        "# architecture (see below), while policy and value functions are linear projections from the encodings. There is\n",
        "# plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala`\n",
        "# encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM).\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "            Flatten(),\n",
        "            nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
        "        )\n",
        "        self.apply(orthogonal_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def loss(self, q_outputs, q_targets):\n",
        "        return F.mse_loss(q_outputs, q_targets)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, encoder, feature_dim, num_actions):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "        self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            dist, value = self.forward(x)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x).squeeze(1)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "        return dist, value\n",
        "\n",
        "\n",
        "def create_and_train_network():\n",
        "    # Define environment\n",
        "    # check the utils.py file for info on arguments\n",
        "    env = make_env(num_envs, num_levels=num_levels)\n",
        "    print('Observation space:', env.observation_space)\n",
        "    print('Action space:', env.action_space.n)\n",
        "    channels_in = env.observation_space.shape[0]\n",
        "    actions = env.action_space.n\n",
        "\n",
        "    # Define network\n",
        "    encoder = Encoder(channels_in, actions)\n",
        "    policy = Policy(encoder, actions, actions)\n",
        "    policy.cuda()\n",
        "\n",
        "    # Define optimizer\n",
        "    # these are reasonable values but probably not optimal\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "    # Define temporary storage\n",
        "    # we use this to collect transitions during each iteration\n",
        "    storage = Storage(\n",
        "        env.observation_space.shape,\n",
        "        num_steps,\n",
        "        num_envs\n",
        "    )\n",
        "\n",
        "    # Run training\n",
        "    obs = env.reset()\n",
        "    step = 0\n",
        "    while step < total_steps:\n",
        "\n",
        "        # Use policy to collect data for num_steps steps\n",
        "        policy.eval()\n",
        "        for _ in range(num_steps):\n",
        "            # Use policy\n",
        "            action, log_prob, value = policy.act(obs)\n",
        "\n",
        "            # Take step in environment\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store data\n",
        "            storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "\n",
        "            # Update current observation\n",
        "            obs = next_obs\n",
        "\n",
        "        # Add the last observation to collected data\n",
        "        _, _, value = policy.act(obs)\n",
        "        storage.store_last(obs, value)\n",
        "\n",
        "        # Compute return and advantage\n",
        "        storage.compute_return_advantage()\n",
        "\n",
        "        # Optimize policy\n",
        "        policy.train()\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            # Iterate over batches of transitions\n",
        "            generator = storage.get_generator(batch_size)\n",
        "            for batch in generator:\n",
        "                b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "                # Get current policy outputs\n",
        "                new_dist, new_value = policy(b_obs)\n",
        "                new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "                # Clipped policy objective\n",
        "                ratio = (new_log_prob - b_log_prob).exp().squeeze(0)\n",
        "                m1 = ratio * b_advantage\n",
        "                m2 = torch.clamp(ratio, 1.0 - eps, 1.0 + eps) * b_advantage\n",
        "                pi_loss = torch.min(m1, m2).mean()\n",
        "\n",
        "                # Clipped value function objective\n",
        "                value_loss = value_coef * encoder.loss(new_value, b_advantage)\n",
        "\n",
        "                # Entropy loss\n",
        "                entropy_loss = entropy_coef * new_dist.entropy().mean()\n",
        "\n",
        "                # Backpropagate losses\n",
        "                loss = - pi_loss + value_loss - entropy_loss  # Eq. 9 in PPO article(https://arxiv.org/pdf/1707.06347.pdf)\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients\n",
        "                torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "                # Update policy\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "\n",
        "        # Update stats\n",
        "        step += num_envs * num_steps\n",
        "        print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "\n",
        "    print('Completed training!')\n",
        "    torch.save(policy.state_dict, 'checkpoint.pt')\n",
        "    return policy\n",
        "\n",
        "\n",
        "# Below cell can be used for policy evaluation and saves an episode to mp4 for you to view.\n",
        "def record_and_eval_policy(policy):\n",
        "    # Make evaluation environment\n",
        "    eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels)\n",
        "    obs = eval_env.reset()\n",
        "\n",
        "    frames = []\n",
        "    total_reward = []\n",
        "\n",
        "    # Evaluate policy\n",
        "    policy.eval()\n",
        "    for _ in range(512):\n",
        "        # Use policy\n",
        "        action, log_prob, value = policy.act(obs)\n",
        "\n",
        "        # Take step in environment\n",
        "        obs, reward, done, info = eval_env.step(action)\n",
        "        total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "        # Render environment and store\n",
        "        frame = (torch.Tensor(eval_env.render(mode='rgb_array')) * 255.).byte()\n",
        "        frames.append(frame)\n",
        "\n",
        "    # Calculate average return\n",
        "    total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "    print('Average return:', total_reward)\n",
        "\n",
        "    # Save frames as video\n",
        "    frames = torch.stack(frames)\n",
        "    imageio.mimsave('vid.mp4', frames, fps=25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXcmc_KEuzd_",
        "outputId": "b0cf8c46-a2b1-42f5-e18e-9455f9401651"
      },
      "source": [
        "complete_policy = create_and_train_network()\n",
        "record_and_eval_policy(complete_policy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n",
            "Step: 8192\tMean reward: 3.75\n",
            "Step: 16384\tMean reward: 4.46875\n",
            "Step: 24576\tMean reward: 3.625\n",
            "Step: 32768\tMean reward: 4.65625\n",
            "Step: 40960\tMean reward: 4.21875\n",
            "Step: 49152\tMean reward: 3.53125\n",
            "Step: 57344\tMean reward: 3.5\n",
            "Step: 65536\tMean reward: 4.84375\n",
            "Step: 73728\tMean reward: 4.78125\n",
            "Step: 81920\tMean reward: 4.65625\n",
            "Step: 90112\tMean reward: 5.59375\n",
            "Step: 98304\tMean reward: 5.28125\n",
            "Step: 106496\tMean reward: 6.96875\n",
            "Step: 114688\tMean reward: 5.3125\n",
            "Step: 122880\tMean reward: 6.59375\n",
            "Step: 131072\tMean reward: 8.0\n",
            "Step: 139264\tMean reward: 6.84375\n",
            "Step: 147456\tMean reward: 8.3125\n",
            "Step: 155648\tMean reward: 8.15625\n",
            "Step: 163840\tMean reward: 7.5625\n",
            "Step: 172032\tMean reward: 9.125\n",
            "Step: 180224\tMean reward: 8.625\n",
            "Step: 188416\tMean reward: 9.84375\n",
            "Step: 196608\tMean reward: 9.8125\n",
            "Step: 204800\tMean reward: 9.03125\n",
            "Step: 212992\tMean reward: 8.96875\n",
            "Step: 221184\tMean reward: 10.84375\n",
            "Step: 229376\tMean reward: 9.1875\n",
            "Step: 237568\tMean reward: 9.75\n",
            "Step: 245760\tMean reward: 9.3125\n",
            "Step: 253952\tMean reward: 10.6875\n",
            "Step: 262144\tMean reward: 9.09375\n",
            "Step: 270336\tMean reward: 9.46875\n",
            "Step: 278528\tMean reward: 10.09375\n",
            "Step: 286720\tMean reward: 9.0625\n",
            "Step: 294912\tMean reward: 10.375\n",
            "Step: 303104\tMean reward: 9.78125\n",
            "Step: 311296\tMean reward: 10.1875\n",
            "Step: 319488\tMean reward: 9.9375\n",
            "Step: 327680\tMean reward: 9.65625\n",
            "Step: 335872\tMean reward: 10.4375\n",
            "Step: 344064\tMean reward: 8.8125\n",
            "Step: 352256\tMean reward: 9.96875\n",
            "Step: 360448\tMean reward: 10.09375\n",
            "Step: 368640\tMean reward: 9.40625\n",
            "Step: 376832\tMean reward: 9.46875\n",
            "Step: 385024\tMean reward: 8.25\n",
            "Step: 393216\tMean reward: 9.5\n",
            "Step: 401408\tMean reward: 9.625\n",
            "Step: 409600\tMean reward: 8.875\n",
            "Step: 417792\tMean reward: 9.71875\n",
            "Step: 425984\tMean reward: 8.53125\n",
            "Step: 434176\tMean reward: 9.21875\n",
            "Step: 442368\tMean reward: 9.03125\n",
            "Step: 450560\tMean reward: 9.90625\n",
            "Step: 458752\tMean reward: 9.6875\n",
            "Step: 466944\tMean reward: 8.0\n",
            "Step: 475136\tMean reward: 9.625\n",
            "Step: 483328\tMean reward: 9.34375\n",
            "Step: 491520\tMean reward: 8.6875\n",
            "Step: 499712\tMean reward: 10.09375\n",
            "Step: 507904\tMean reward: 10.59375\n",
            "Step: 516096\tMean reward: 10.4375\n",
            "Step: 524288\tMean reward: 9.0625\n",
            "Step: 532480\tMean reward: 10.6875\n",
            "Step: 540672\tMean reward: 11.34375\n",
            "Step: 548864\tMean reward: 9.1875\n",
            "Step: 557056\tMean reward: 9.875\n",
            "Step: 565248\tMean reward: 9.0\n",
            "Step: 573440\tMean reward: 10.71875\n",
            "Step: 581632\tMean reward: 8.65625\n",
            "Step: 589824\tMean reward: 8.9375\n",
            "Step: 598016\tMean reward: 9.5625\n",
            "Step: 606208\tMean reward: 9.375\n",
            "Step: 614400\tMean reward: 9.75\n",
            "Step: 622592\tMean reward: 9.75\n",
            "Step: 630784\tMean reward: 9.625\n",
            "Step: 638976\tMean reward: 10.375\n",
            "Step: 647168\tMean reward: 9.03125\n",
            "Step: 655360\tMean reward: 9.875\n",
            "Step: 663552\tMean reward: 10.34375\n",
            "Step: 671744\tMean reward: 9.4375\n",
            "Step: 679936\tMean reward: 9.65625\n",
            "Step: 688128\tMean reward: 11.09375\n",
            "Step: 696320\tMean reward: 10.34375\n",
            "Step: 704512\tMean reward: 9.9375\n",
            "Step: 712704\tMean reward: 10.40625\n",
            "Step: 720896\tMean reward: 9.90625\n",
            "Step: 729088\tMean reward: 11.46875\n",
            "Step: 737280\tMean reward: 10.625\n",
            "Step: 745472\tMean reward: 10.46875\n",
            "Step: 753664\tMean reward: 9.78125\n",
            "Step: 761856\tMean reward: 10.46875\n",
            "Step: 770048\tMean reward: 11.59375\n",
            "Step: 778240\tMean reward: 11.46875\n",
            "Step: 786432\tMean reward: 11.96875\n",
            "Step: 794624\tMean reward: 11.34375\n",
            "Step: 802816\tMean reward: 10.40625\n",
            "Step: 811008\tMean reward: 12.59375\n",
            "Step: 819200\tMean reward: 12.59375\n",
            "Step: 827392\tMean reward: 11.3125\n",
            "Step: 835584\tMean reward: 11.78125\n",
            "Step: 843776\tMean reward: 11.5625\n",
            "Step: 851968\tMean reward: 13.03125\n",
            "Step: 860160\tMean reward: 11.6875\n",
            "Step: 868352\tMean reward: 11.375\n",
            "Step: 876544\tMean reward: 13.1875\n",
            "Step: 884736\tMean reward: 14.75\n",
            "Step: 892928\tMean reward: 12.3125\n",
            "Step: 901120\tMean reward: 13.71875\n",
            "Step: 909312\tMean reward: 12.53125\n",
            "Step: 917504\tMean reward: 15.0625\n",
            "Step: 925696\tMean reward: 13.25\n",
            "Step: 933888\tMean reward: 12.875\n",
            "Step: 942080\tMean reward: 12.78125\n",
            "Step: 950272\tMean reward: 12.46875\n",
            "Step: 958464\tMean reward: 13.3125\n",
            "Step: 966656\tMean reward: 14.125\n",
            "Step: 974848\tMean reward: 15.15625\n",
            "Step: 983040\tMean reward: 15.4375\n",
            "Step: 991232\tMean reward: 15.03125\n",
            "Step: 999424\tMean reward: 13.96875\n",
            "Step: 1007616\tMean reward: 14.40625\n",
            "Step: 1015808\tMean reward: 14.75\n",
            "Step: 1024000\tMean reward: 15.375\n",
            "Step: 1032192\tMean reward: 15.6875\n",
            "Step: 1040384\tMean reward: 15.71875\n",
            "Step: 1048576\tMean reward: 14.90625\n",
            "Step: 1056768\tMean reward: 15.0\n",
            "Step: 1064960\tMean reward: 15.09375\n",
            "Step: 1073152\tMean reward: 15.125\n",
            "Step: 1081344\tMean reward: 13.3125\n",
            "Step: 1089536\tMean reward: 15.0\n",
            "Step: 1097728\tMean reward: 14.84375\n",
            "Step: 1105920\tMean reward: 13.28125\n",
            "Step: 1114112\tMean reward: 14.8125\n",
            "Step: 1122304\tMean reward: 14.03125\n",
            "Step: 1130496\tMean reward: 16.03125\n",
            "Step: 1138688\tMean reward: 15.0\n",
            "Step: 1146880\tMean reward: 13.5625\n",
            "Step: 1155072\tMean reward: 16.21875\n",
            "Step: 1163264\tMean reward: 14.90625\n",
            "Step: 1171456\tMean reward: 14.84375\n",
            "Step: 1179648\tMean reward: 16.125\n",
            "Step: 1187840\tMean reward: 15.59375\n",
            "Step: 1196032\tMean reward: 14.8125\n",
            "Step: 1204224\tMean reward: 16.9375\n",
            "Step: 1212416\tMean reward: 16.0625\n",
            "Step: 1220608\tMean reward: 15.53125\n",
            "Step: 1228800\tMean reward: 15.65625\n",
            "Step: 1236992\tMean reward: 15.40625\n",
            "Step: 1245184\tMean reward: 15.21875\n",
            "Step: 1253376\tMean reward: 14.65625\n",
            "Step: 1261568\tMean reward: 17.125\n",
            "Step: 1269760\tMean reward: 15.84375\n",
            "Step: 1277952\tMean reward: 16.84375\n",
            "Step: 1286144\tMean reward: 14.78125\n",
            "Step: 1294336\tMean reward: 16.4375\n",
            "Step: 1302528\tMean reward: 15.71875\n",
            "Step: 1310720\tMean reward: 17.375\n",
            "Step: 1318912\tMean reward: 15.34375\n",
            "Step: 1327104\tMean reward: 15.375\n",
            "Step: 1335296\tMean reward: 17.0\n",
            "Step: 1343488\tMean reward: 17.34375\n",
            "Step: 1351680\tMean reward: 17.21875\n",
            "Step: 1359872\tMean reward: 17.09375\n",
            "Step: 1368064\tMean reward: 17.0625\n",
            "Step: 1376256\tMean reward: 16.84375\n",
            "Step: 1384448\tMean reward: 17.28125\n",
            "Step: 1392640\tMean reward: 17.71875\n",
            "Step: 1400832\tMean reward: 17.21875\n",
            "Step: 1409024\tMean reward: 16.03125\n",
            "Step: 1417216\tMean reward: 16.40625\n",
            "Step: 1425408\tMean reward: 16.8125\n",
            "Step: 1433600\tMean reward: 16.125\n",
            "Step: 1441792\tMean reward: 15.71875\n",
            "Step: 1449984\tMean reward: 16.40625\n",
            "Step: 1458176\tMean reward: 17.9375\n",
            "Step: 1466368\tMean reward: 17.71875\n",
            "Step: 1474560\tMean reward: 16.15625\n",
            "Step: 1482752\tMean reward: 18.96875\n",
            "Step: 1490944\tMean reward: 16.71875\n",
            "Step: 1499136\tMean reward: 17.5625\n",
            "Step: 1507328\tMean reward: 17.3125\n",
            "Step: 1515520\tMean reward: 18.46875\n",
            "Step: 1523712\tMean reward: 17.59375\n",
            "Step: 1531904\tMean reward: 18.40625\n",
            "Step: 1540096\tMean reward: 18.78125\n",
            "Step: 1548288\tMean reward: 18.5\n",
            "Step: 1556480\tMean reward: 18.9375\n",
            "Step: 1564672\tMean reward: 18.6875\n",
            "Step: 1572864\tMean reward: 18.5\n",
            "Step: 1581056\tMean reward: 16.9375\n",
            "Step: 1589248\tMean reward: 18.53125\n",
            "Step: 1597440\tMean reward: 18.84375\n",
            "Step: 1605632\tMean reward: 17.8125\n",
            "Step: 1613824\tMean reward: 18.40625\n",
            "Step: 1622016\tMean reward: 17.90625\n",
            "Step: 1630208\tMean reward: 18.90625\n",
            "Step: 1638400\tMean reward: 17.09375\n",
            "Step: 1646592\tMean reward: 17.78125\n",
            "Step: 1654784\tMean reward: 17.375\n",
            "Step: 1662976\tMean reward: 19.125\n",
            "Step: 1671168\tMean reward: 18.0\n",
            "Step: 1679360\tMean reward: 18.125\n",
            "Step: 1687552\tMean reward: 18.6875\n",
            "Step: 1695744\tMean reward: 19.75\n",
            "Step: 1703936\tMean reward: 17.09375\n",
            "Step: 1712128\tMean reward: 17.0625\n",
            "Step: 1720320\tMean reward: 18.125\n",
            "Step: 1728512\tMean reward: 18.78125\n",
            "Step: 1736704\tMean reward: 18.0\n",
            "Step: 1744896\tMean reward: 19.125\n",
            "Step: 1753088\tMean reward: 19.28125\n",
            "Step: 1761280\tMean reward: 19.5\n",
            "Step: 1769472\tMean reward: 17.8125\n",
            "Step: 1777664\tMean reward: 19.3125\n",
            "Step: 1785856\tMean reward: 18.875\n",
            "Step: 1794048\tMean reward: 18.34375\n",
            "Step: 1802240\tMean reward: 17.8125\n",
            "Step: 1810432\tMean reward: 19.59375\n",
            "Step: 1818624\tMean reward: 19.75\n",
            "Step: 1826816\tMean reward: 18.875\n",
            "Step: 1835008\tMean reward: 18.375\n",
            "Step: 1843200\tMean reward: 18.875\n",
            "Step: 1851392\tMean reward: 20.90625\n",
            "Step: 1859584\tMean reward: 18.21875\n",
            "Step: 1867776\tMean reward: 19.03125\n",
            "Step: 1875968\tMean reward: 19.9375\n",
            "Step: 1884160\tMean reward: 18.6875\n",
            "Step: 1892352\tMean reward: 19.9375\n",
            "Step: 1900544\tMean reward: 18.71875\n",
            "Step: 1908736\tMean reward: 18.625\n",
            "Step: 1916928\tMean reward: 19.8125\n",
            "Step: 1925120\tMean reward: 20.53125\n",
            "Step: 1933312\tMean reward: 18.28125\n",
            "Step: 1941504\tMean reward: 19.71875\n",
            "Step: 1949696\tMean reward: 19.4375\n",
            "Step: 1957888\tMean reward: 19.09375\n",
            "Step: 1966080\tMean reward: 19.84375\n",
            "Step: 1974272\tMean reward: 20.96875\n",
            "Step: 1982464\tMean reward: 20.40625\n",
            "Step: 1990656\tMean reward: 18.59375\n",
            "Step: 1998848\tMean reward: 20.53125\n",
            "Step: 2007040\tMean reward: 20.375\n",
            "Step: 2015232\tMean reward: 18.90625\n",
            "Step: 2023424\tMean reward: 20.4375\n",
            "Step: 2031616\tMean reward: 19.46875\n",
            "Step: 2039808\tMean reward: 21.03125\n",
            "Step: 2048000\tMean reward: 19.8125\n",
            "Step: 2056192\tMean reward: 20.9375\n",
            "Step: 2064384\tMean reward: 21.0\n",
            "Step: 2072576\tMean reward: 20.59375\n",
            "Step: 2080768\tMean reward: 19.125\n",
            "Step: 2088960\tMean reward: 20.75\n",
            "Step: 2097152\tMean reward: 19.5625\n",
            "Step: 2105344\tMean reward: 19.9375\n",
            "Step: 2113536\tMean reward: 20.59375\n",
            "Step: 2121728\tMean reward: 21.15625\n",
            "Step: 2129920\tMean reward: 20.5\n",
            "Step: 2138112\tMean reward: 20.9375\n",
            "Step: 2146304\tMean reward: 20.40625\n",
            "Step: 2154496\tMean reward: 21.28125\n",
            "Step: 2162688\tMean reward: 19.875\n",
            "Step: 2170880\tMean reward: 21.625\n",
            "Step: 2179072\tMean reward: 20.65625\n",
            "Step: 2187264\tMean reward: 21.3125\n",
            "Step: 2195456\tMean reward: 20.71875\n",
            "Step: 2203648\tMean reward: 20.84375\n",
            "Step: 2211840\tMean reward: 20.53125\n",
            "Step: 2220032\tMean reward: 20.90625\n",
            "Step: 2228224\tMean reward: 21.625\n",
            "Step: 2236416\tMean reward: 20.21875\n"
          ]
        }
      ]
    }
  ]
}